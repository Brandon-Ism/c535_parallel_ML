{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Le46Cx7zYku0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# # Verify the file path\n",
        "# file_path = r\"Toys_and_Games.json\"\n",
        "# if not os.path.exists(file_path):\n",
        "#     raise FileNotFoundError(f\"The file at {file_path} does not exist.\")\n",
        "\n",
        "# # Read the file in chunks\n",
        "# chunk_size = 10000  # Adjust the chunk size based on your memory constraints\n",
        "# chunks = pd.read_json(file_path, lines=True, chunksize=chunk_size)\n",
        "\n",
        "# # List to store processed chunks\n",
        "# processed_chunks = []\n",
        "\n",
        "# # Process each chunk\n",
        "# for chunk in chunks:\n",
        "#     # Drop all columns except for 'reviewText' and 'class'\n",
        "#     chunk = chunk[['reviewText', 'class']]\n",
        "\n",
        "#     # Perform necessary operations on each chunk\n",
        "#     print(chunk.head())   # Check the first few rows of the chunk\n",
        "#     print(chunk.columns)  # Check the column names\n",
        "#     print(chunk.info())   # Get a summary of the data types and null values\n",
        "\n",
        "#     # Example: Convert data types to more memory-efficient types\n",
        "#     chunk['class'] = chunk['class'].astype('category')\n",
        "\n",
        "#     # Append the processed chunk to the list\n",
        "#     processed_chunks.append(chunk)\n",
        "\n",
        "# # Combine all processed chunks into a single DataFrame\n",
        "# final_df = pd.concat(processed_chunks, ignore_index=True)\n",
        "\n",
        "# # Save the final DataFrame to a new file\n",
        "# final_df.to_csv('processed_reviews.csv', index=False)\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "final_df = pd.read_csv('processed_reviews.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Preprocess the data\n",
        "final_df.dropna(inplace=True)\n",
        "X = final_df[\"reviewText\"]\n",
        "y = final_df['class']  # Assuming 'class' is your target label (fake or real review)\n",
        "\n",
        "# 1. Use TF-IDF with a limited number of features to reduce memory usage\n",
        "vectorizer = TfidfVectorizer(max_features=15000)  # Limit to 15000 features\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize the training and testing sets\n",
        "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "X_test_vectorized = vectorizer.transform(X_test)\n",
        "\n",
        "# Custom dataset class to handle sparse tensors\n",
        "class SparseDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        X_row = self.X[idx].toarray().squeeze()\n",
        "        y_row = self.y[idx]\n",
        "        return torch.tensor(X_row, dtype=torch.float32), torch.tensor(y_row, dtype=torch.float32)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = SparseDataset(X_train_vectorized, y_train.values)\n",
        "test_dataset = SparseDataset(X_test_vectorized, y_test.values)\n",
        "\n",
        "# Custom collate function to handle sparse tensors\n",
        "def sparse_collate_fn(batch):\n",
        "    X_batch, y_batch = zip(*batch)\n",
        "    X_batch = torch.stack(X_batch)\n",
        "    y_batch = torch.stack(y_batch).view(-1, 1)  # Reshape y_batch to have the same shape as y_pred\n",
        "    return X_batch.cuda(), y_batch.cuda()\n",
        "\n",
        "# Use DataLoader for efficient batch processing on GPU with custom collate_fn\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=sparse_collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=sparse_collate_fn)\n",
        "\n",
        "# Model (assuming there's a simple neural network to classify reviews)\n",
        "class SpamReviewClassifier(torch.nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(SpamReviewClassifier, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(input_size, 128)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.fc2 = torch.nn.Linear(128, 1)\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "# Initialize the model and move it to GPU\n",
        "model = SpamReviewClassifier(input_size=15000).cuda()\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = torch.nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(X_batch)\n",
        "        loss = criterion(y_pred, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhVWZajkZJCS",
        "outputId": "13d4988c-b313-4cd4-8ad7-17228fafff3d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Loss: 0.0431184396147728\n",
            "Epoch 2/5, Loss: 0.07764448970556259\n",
            "Epoch 3/5, Loss: 0.2717161774635315\n",
            "Epoch 4/5, Loss: 0.4241695702075958\n",
            "Epoch 5/5, Loss: 0.1997235119342804\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation function\n",
        "def evaluate(model, test_loader, criterion):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            y_pred = model(X_batch)\n",
        "            test_loss += criterion(y_pred, y_batch).item()\n",
        "            pred = (y_pred > 0.5).float()  # Convert probabilities to binary predictions\n",
        "            correct += pred.eq(y_batch).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    accuracy = correct / len(test_loader.dataset)\n",
        "    return test_loss, accuracy\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "test_loss, test_accuracy = evaluate(model, test_loader, criterion)\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yg7rGLs-mT91",
        "outputId": "047ad9a2-7be0-4410-e35c-96b3386fa15e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.0066, Test Accuracy: 0.9187\n"
          ]
        }
      ]
    }
  ]
}