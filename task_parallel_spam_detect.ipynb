{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Logistic Regression Model for Spam Detection of Amazon \"Sports and Outdoors\" Product Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook is to implement task parallelism, along with the current implementation of data parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import multiprocessing as mp\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress all FutureWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data = pd.read_json('~/Documents/Sports_and_Outdoors/Sports_and_Outdoors.json', lines=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the input features will be: `reviewText`, `overall`, `summary`, and `helpful`\n",
    "The predictor will be `class`, which indicates whether the review is spam (1), or not spam (0)\n",
    "\n",
    "The first element of the `helpful` feature is extracted, indicating the number of users that found that review helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the relevant columns\n",
    "data = data[['reviewText', 'overall', 'summary', 'helpful', 'class']]\n",
    "\n",
    "# Clean the 'helpful' column: extract the first element of the list - num of helpful votes\n",
    "data['helpful'] = data['helpful'].apply(lambda x: x[0] if isinstance(x, list) and len(x) > 0 else 0)\n",
    "\n",
    "# Check cleaned data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[['reviewText', 'overall', 'summary', 'helpful']], \n",
    "                                                    data['class'], test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must convert text features (`reviewText` & `summary`) into numerical vectors suitable for ML training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define separate functions for vectorizing and scaling our text features, into formats suitable for machine learning processing. \n",
    "These separate functions will be run in parallel, to vectorize and scale each of these features in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to vectorize 'reviewText' feature\n",
    "def vectorize_review_text(X_train, X_test):\n",
    "    vectorizer_review = TfidfVectorizer(max_features=5000)\n",
    "    X_train_review_tfidf = vectorizer_review.fit_transform(X_train['reviewText'])\n",
    "    X_test_review_tfidf = vectorizer_review.transform(X_test['reviewText'])\n",
    "    return X_train_review_tfidf, X_test_review_tfidf\n",
    "\n",
    "# Function to vectorize 'summary' feature\n",
    "def vectorize_summary(X_train, X_test):\n",
    "    vectorizer_summary = TfidfVectorizer(max_features=1000)\n",
    "    X_train_summary_tfidf = vectorizer_summary.fit_transform(X_train['summary'])\n",
    "    X_test_summary_tfidf = vectorizer_summary.transform(X_test['summary'])\n",
    "    return X_train_summary_tfidf, X_test_summary_tfidf\n",
    "\n",
    "# Function to scale 'overall' and 'helpful' features\n",
    "def scale_numerical_features(X_train, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train[['overall', 'helpful']])\n",
    "    X_test_scaled = scaler.transform(X_test[['overall', 'helpful']])\n",
    "    return X_train_scaled, X_test_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the task parallelism is implemented, where we will use `ThreadPoolExecutor` to perform our preprocessing tasks in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer for parallel data preprocessing\n",
    "start_time = time.time()\n",
    "\n",
    "#  Execute tasks in parallel using ThreadPoolExecutor\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    future_review = executor.submit(vectorize_review_text, X_train, X_test)\n",
    "    future_summary = executor.submit(vectorize_summary, X_train, X_test)\n",
    "    future_scaling = executor.submit(scale_numerical_features, X_train, X_test)\n",
    "\n",
    "\n",
    "    X_train_review_tfidf, X_test_review_tfidf = future_review.result()\n",
    "    X_train_summary_tfidf, X_test_summary_tfidf = future_summary.result()\n",
    "    X_train_scaled, X_test_scaled = future_scaling.result()\n",
    "\n",
    "# Stop timer\n",
    "end_time = time.time()\n",
    "print(f\"Data preprocessing (task parallelism) completed in: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessed features are then combined and checked to ensure that they are appropriate dimensions and formats to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Combine training and testing features\n",
    "X_train_combined = hstack([X_train_review_tfidf, X_train_summary_tfidf, csr_matrix(X_train_scaled)])\n",
    "X_test_combined = hstack([X_test_review_tfidf, X_test_summary_tfidf, csr_matrix(X_test_scaled)])\n",
    "\n",
    "\n",
    "# Check final shapes to make sure dimensions are compatible\n",
    "print(f\"Shape of X_train_combined: {X_train_combined.shape}\")\n",
    "print(f\"Shape of X_test_combined: {X_test_combined.shape}\")\n",
    "\n",
    "\n",
    "# Convert combined matrix to CSR format for slicing\n",
    "X_train_combined = X_train_combined.tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we wil define and split the data into `num_chunks` chunks, where `num_chunks` can be equal to the number of CPU cores or be manually chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of data chunks\n",
    "num_chunks = mp.cpu_count()\n",
    "print(f\"Number of chunks utilized: {num_chunks}\")\n",
    "\n",
    "# Calculate chunk size\n",
    "chunk_size = X_train_combined.shape[0] // num_chunks\n",
    "\n",
    "# Ensure the last chunk includes all remaining rows if the split is not even\n",
    "X_train_chunks = [X_train_combined[i*chunk_size:(i+1)*chunk_size] for i in range(num_chunks-1)]\n",
    "y_train_chunks = [y_train[i*chunk_size:(i+1)*chunk_size] for i in range(num_chunks-1)]\n",
    "\n",
    "# Add remaining rows in the last chunk\n",
    "X_train_chunks.append(X_train_combined[(num_chunks-1)*chunk_size:])\n",
    "y_train_chunks.append(y_train[(num_chunks-1)*chunk_size:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The defined function below will train the Logistic Regression model on each 'chunk' of the data in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to train Logistic Regression on a chunk of data\n",
    "def train_on_chunk(X_chunk, y_chunk):\n",
    "    # Ensure data is writable\n",
    "    X_chunk = X_chunk.copy()\n",
    "    y_chunk = y_chunk.copy()\n",
    "\n",
    "    model = LogisticRegression(max_iter=1000, solver='lbfgs')\n",
    "    model.fit(X_chunk, y_chunk)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we train the the model in parallel, and time the training time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Create a multiprocessing pool\n",
    "with mp.Pool(processes=num_chunks) as pool:\n",
    "    # Train models in parallel on each chunk of data\n",
    "    models = pool.starmap(train_on_chunk, zip(X_train_chunks, y_train_chunks))\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the parallel training time\n",
    "parallel_training_time = end_time - start_time\n",
    "print(f\"Parallel Training Time: {parallel_training_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
