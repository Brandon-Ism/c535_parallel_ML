{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2trMojzDtqJE",
        "outputId": "92fe4a97-8678-447f-9212-193134daf80d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Nov  6 23:03:18 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1DFhq8e0cSy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "final_df = pd.read_csv('processed_reviews.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLy0mDdq2WjY"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from torch.utils.data import DataLoader, Dataset\n",
        "# import torch\n",
        "# import numpy as np\n",
        "\n",
        "# # Read the CSV file into a DataFrame\n",
        "# final_df = pd.read_csv('processed_reviews.csv')\n",
        "\n",
        "# # Preprocess the data\n",
        "# final_df.dropna(inplace=True)\n",
        "# X = final_df[\"reviewText\"]\n",
        "# y = final_df['class']  # Assuming 'class' is your target label (fake or real review)\n",
        "\n",
        "# # 1. Use TF-IDF with a limited number of features to reduce memory usage\n",
        "# vectorizer = TfidfVectorizer(max_features=15000)  # Limit to 15000 features\n",
        "\n",
        "# # Split the data into training and testing sets\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# # Vectorize the training and testing sets\n",
        "# X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "# X_test_vectorized = vectorizer.transform(X_test)\n",
        "\n",
        "# # Custom dataset class to handle sparse tensors\n",
        "# class SparseDataset(Dataset):\n",
        "#     def __init__(self, X, y):\n",
        "#         self.X = X\n",
        "#         self.y = y\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return self.X.shape[0]\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         X_row = self.X[idx].toarray().squeeze()\n",
        "#         y_row = self.y[idx]\n",
        "#         return torch.tensor(X_row, dtype=torch.float32), torch.tensor(y_row, dtype=torch.float32)\n",
        "\n",
        "# # Create datasets\n",
        "# train_dataset = SparseDataset(X_train_vectorized, y_train.values)\n",
        "# test_dataset = SparseDataset(X_test_vectorized, y_test.values)\n",
        "\n",
        "# # Custom collate function to handle sparse tensors\n",
        "# def sparse_collate_fn(batch):\n",
        "#     X_batch, y_batch = zip(*batch)\n",
        "#     X_batch = torch.stack(X_batch)\n",
        "#     y_batch = torch.stack(y_batch).view(-1, 1)  # Reshape y_batch to have the same shape as y_pred\n",
        "#     return X_batch.cuda(), y_batch.cuda()\n",
        "\n",
        "# # Use DataLoader for efficient batch processing on GPU with custom collate_fn\n",
        "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=sparse_collate_fn)\n",
        "# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=sparse_collate_fn)\n",
        "\n",
        "# # Simplified model\n",
        "# class SimpleSpamReviewClassifier(torch.nn.Module):\n",
        "#     def __init__(self, input_size):\n",
        "#         super(SimpleSpamReviewClassifier, self).__init__()\n",
        "#         self.fc1 = torch.nn.Linear(input_size, 1)\n",
        "#         self.sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.fc1(x)\n",
        "#         return self.sigmoid(x)\n",
        "\n",
        "# # Initialize the model and move it to GPU\n",
        "# model = SimpleSpamReviewClassifier(input_size=15000).cuda()\n",
        "\n",
        "# # Loss and optimizer\n",
        "# criterion = torch.nn.BCELoss()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# # Training loop\n",
        "# epochs = 5\n",
        "# for epoch in range(epochs):\n",
        "#     model.train()\n",
        "#     for X_batch, y_batch in train_loader:\n",
        "#         optimizer.zero_grad()\n",
        "#         y_pred = model(X_batch)\n",
        "#         loss = criterion(y_pred, y_batch)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#     print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "53d32a99"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import time\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "final_df = pd.read_csv('processed_reviews.csv')\n",
        "\n",
        "# Preprocess the data\n",
        "final_df.dropna(inplace=True)\n",
        "X = final_df[\"reviewText\"]\n",
        "y = final_df['class']  # Assuming 'class' is your target label (fake or real review)\n",
        "\n",
        "# Use TF-IDF with a limited number of features to reduce memory usage\n",
        "vectorizer = TfidfVectorizer(max_features=15000)\n",
        "X_vectorized = vectorizer.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.2, random_state=42)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom dataset class to handle sparse tensors\n",
        "class SparseDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]\n",
        "    def __getitem__(self, idx):\n",
        "        X_row = self.X[idx].toarray().squeeze()\n",
        "        y_row = self.y[idx]\n",
        "        # Remove the view(-1, 1) to keep the target tensor as a single value\n",
        "        return torch.tensor(X_row, dtype=torch.float32), torch.tensor(y_row, dtype=torch.float32)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = SparseDataset(X_train, y_train.values)\n",
        "test_dataset = SparseDataset(X_test, y_test.values)\n",
        "\n",
        "# Use DataLoader for efficient batch processing\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define a simple model\n",
        "class SimpleSpamReviewClassifier(torch.nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(SimpleSpamReviewClassifier, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(input_size, 1)\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "# Initialize the model\n",
        "model = SimpleSpamReviewClassifier(input_size=15000)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = torch.nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Start time for runtime calculation\n",
        "start_time = time.time()\n",
        "\n",
        "# Sequential training loop\n",
        "# Training loop\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        optimizer.zero_grad()  # Reset gradients\n",
        "\n",
        "        # Forward pass\n",
        "        y_pred = model(X_batch)\n",
        "\n",
        "        # Reshape y_batch to match y_pred's shape if necessary\n",
        "        # y_batch = y_batch.unsqueeze(1)  # Add an extra dimension if y_batch is 1D\n",
        "\n",
        "        loss = criterion(y_pred, y_batch.unsqueeze(1)) # Add unsqueeze(1) here\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "# Convert runtime to minutes and seconds\n",
        "runtime = time.time() - start_time\n",
        "minutes = int(runtime // 60)\n",
        "seconds = int(runtime % 60)\n",
        "\n",
        "\n",
        "# Evaluation on test set\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        y_test_pred = model(X_batch)\n",
        "\n",
        "        # Reshape y_batch to match the dimensions of y_test_pred\n",
        "        test_loss += criterion(y_test_pred, y_batch.unsqueeze(1)).item()\n",
        "\n",
        "        predictions = (y_test_pred > 0.5).float()\n",
        "        correct += (predictions.eq(y_batch.unsqueeze(1)).sum().item())\n",
        "        total += y_batch.size(0)\n",
        "\n",
        "test_loss /= len(test_loader)\n",
        "accuracy = correct / total\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Total Runtime: {minutes} minutes and {seconds} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpBNG8QOk8N4",
        "outputId": "38149585-6d35-48d8-acd1-d72ee0036281"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Loss: 0.3239907920360565\n",
            "Epoch 2/5, Loss: 0.12389504164457321\n",
            "Epoch 3/5, Loss: 0.3149122893810272\n",
            "Epoch 4/5, Loss: 0.21479551494121552\n",
            "Epoch 5/5, Loss: 0.2729133367538452\n",
            "Test Loss: 0.2139, Test Accuracy: 0.9153\n",
            "Total Runtime: 33 minutes and 51 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "saeK--XA68aY"
      },
      "outputs": [],
      "source": [
        "# Evaluation function\n",
        "def evaluate(model, test_loader, criterion):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            y_pred = model(X_batch)\n",
        "            test_loss += criterion(y_pred, y_batch).item()\n",
        "            pred = (y_pred > 0.5).float()  # Convert probabilities to binary predictions\n",
        "            correct += pred.eq(y_batch).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    accuracy = correct / len(test_loader.dataset)\n",
        "    return test_loss, accuracy\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "test_loss, test_accuracy = evaluate(model, test_loader, criterion)\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Garbage Collector - use it like gc.collect()\n",
        "import gc\n",
        "\n",
        "# Custom Callback To Include in Callbacks List At Training Time\n",
        "class GarbageCollectorCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        gc.collect()"
      ],
      "metadata": {
        "id": "CjkIecJF7MaB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}